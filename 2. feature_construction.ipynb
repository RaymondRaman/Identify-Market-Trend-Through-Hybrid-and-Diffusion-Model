{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from sklearn.impute import KNNImputer\n",
    "from general_used_functions import *\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config data\n",
    "config_data = load_config_file()\n",
    "\n",
    "# Load the data from the data directory\n",
    "DATA_DIR = os.getcwd() + '/data'\n",
    "training_stock_df = defaultdict(list)\n",
    "testing_stock_df = defaultdict(list)\n",
    "stock_list = config_data['stock_dict'].keys()\n",
    "\n",
    "# Load traininig data\n",
    "for stock in stock_list:\n",
    "    training_stock_data = pd.read_excel(f\"{DATA_DIR}/stock_price_data/training/{stock}_stock_price_data(training).xlsx\")\n",
    "    training_stock_df[stock] = training_stock_data\n",
    "\n",
    "# Load testing data\n",
    "for stock in stock_list:\n",
    "    testing_stock_data = pd.read_excel(f\"{DATA_DIR}/stock_price_data/testing/{stock}_stock_price_data(testing).xlsx\")\n",
    "    testing_stock_df[stock] = testing_stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature dataframe and store the date as the first column\n",
    "training_feature_df = {}\n",
    "testing_feature_df = {}\n",
    "\n",
    "# Training\n",
    "for stock in stock_list:\n",
    "    training_feature_df[stock] = pd.DataFrame()\n",
    "    training_feature_df[stock]['date'] = training_stock_df[stock]['date']\n",
    "\n",
    "# Testing\n",
    "for stock in stock_list:\n",
    "    testing_feature_df[stock] = pd.DataFrame()\n",
    "    testing_feature_df[stock]['date'] = testing_stock_df[stock]['date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 1 - 3: Exponential Weighted Moving (EMW) with halflives(days) = 5, 10 and 21 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_moving_average(data, halflives):\n",
    "    return data.ewm(halflife=halflives).mean()\n",
    "\n",
    "# Training\n",
    "for stock in stock_list:\n",
    "    for halflife in [5, 10, 21]:\n",
    "        training_feature_df[stock][f'{stock}_exponential_moving_average_{halflife}'] = exponential_moving_average(training_stock_df[stock][stock], halflife)\n",
    "\n",
    "        if check_weird_data(training_feature_df[stock][f'{stock}_exponential_moving_average_{halflife}']):\n",
    "            print(f\"Feature {stock}_exponential_moving_average_{halflife} contains weird data in training data\")\n",
    "\n",
    "# Testing\n",
    "for stock in stock_list:\n",
    "    for halflife in [5, 10, 21]:\n",
    "        testing_feature_df[stock][f'{stock}_exponential_moving_average_{halflife}'] = exponential_moving_average(testing_stock_df[stock][stock], halflife)\n",
    "\n",
    "        if check_weird_data(testing_feature_df[stock][f'{stock}_exponential_moving_average_{halflife}']):\n",
    "            print(f\"Feature {stock}_exponential_moving_average_{halflife} contains weird data in testing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025\n"
     ]
    }
   ],
   "source": [
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 4 - 6: EWM downside deviation (DD) in log scale with halflives(days) = 5, 10, 21 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature AMZN_downside_deviation_5 contains weird data\n",
      "Feature AMZN_downside_deviation_10 contains weird data\n",
      "Feature AMZN_downside_deviation_21 contains weird data\n",
      "Feature GOOGL_downside_deviation_5 contains weird data\n",
      "Feature GOOGL_downside_deviation_10 contains weird data\n",
      "Feature GOOGL_downside_deviation_21 contains weird data\n",
      "Feature MSFT_downside_deviation_5 contains weird data\n",
      "Feature MSFT_downside_deviation_10 contains weird data\n",
      "Feature MSFT_downside_deviation_21 contains weird data\n",
      "Feature NVDA_downside_deviation_5 contains weird data\n",
      "Feature NVDA_downside_deviation_10 contains weird data\n",
      "Feature NVDA_downside_deviation_21 contains weird data\n",
      "Feature AAPL_downside_deviation_5 contains weird data\n",
      "Feature AAPL_downside_deviation_10 contains weird data\n",
      "Feature AAPL_downside_deviation_21 contains weird data\n",
      "Feature NFLX_downside_deviation_5 contains weird data\n",
      "Feature NFLX_downside_deviation_10 contains weird data\n",
      "Feature NFLX_downside_deviation_21 contains weird data\n",
      "Feature AVGO_downside_deviation_5 contains weird data\n",
      "Feature AVGO_downside_deviation_10 contains weird data\n",
      "Feature AVGO_downside_deviation_21 contains weird data\n",
      "Feature TSLA_downside_deviation_5 contains weird data\n",
      "Feature TSLA_downside_deviation_10 contains weird data\n",
      "Feature TSLA_downside_deviation_21 contains weird data\n",
      "Feature META_downside_deviation_5 contains weird data\n",
      "Feature META_downside_deviation_10 contains weird data\n",
      "Feature META_downside_deviation_21 contains weird data\n",
      "Feature AMZN_downside_deviation_5 contains weird data\n",
      "Feature AMZN_downside_deviation_10 contains weird data\n",
      "Feature AMZN_downside_deviation_21 contains weird data\n",
      "Feature GOOGL_downside_deviation_5 contains weird data\n",
      "Feature GOOGL_downside_deviation_10 contains weird data\n",
      "Feature GOOGL_downside_deviation_21 contains weird data\n",
      "Feature MSFT_downside_deviation_5 contains weird data\n",
      "Feature MSFT_downside_deviation_10 contains weird data\n",
      "Feature MSFT_downside_deviation_21 contains weird data\n",
      "Feature NVDA_downside_deviation_5 contains weird data\n",
      "Feature NVDA_downside_deviation_10 contains weird data\n",
      "Feature NVDA_downside_deviation_21 contains weird data\n",
      "Feature AAPL_downside_deviation_5 contains weird data\n",
      "Feature AAPL_downside_deviation_10 contains weird data\n",
      "Feature AAPL_downside_deviation_21 contains weird data\n",
      "Feature NFLX_downside_deviation_5 contains weird data\n",
      "Feature NFLX_downside_deviation_10 contains weird data\n",
      "Feature NFLX_downside_deviation_21 contains weird data\n",
      "Feature AVGO_downside_deviation_5 contains weird data\n",
      "Feature AVGO_downside_deviation_10 contains weird data\n",
      "Feature AVGO_downside_deviation_21 contains weird data\n",
      "Feature TSLA_downside_deviation_5 contains weird data\n",
      "Feature TSLA_downside_deviation_10 contains weird data\n",
      "Feature TSLA_downside_deviation_21 contains weird data\n",
      "Feature META_downside_deviation_5 contains weird data\n",
      "Feature META_downside_deviation_10 contains weird data\n",
      "Feature META_downside_deviation_21 contains weird data\n"
     ]
    }
   ],
   "source": [
    "def weighted_moving_downside_deviation_with_log(data, halflives, TRADING_DAYS=252):\n",
    "    log_returns = np.log(data / data.shift(1))\n",
    "    downside_returns = log_returns[log_returns < 0].fillna(0)\n",
    "    ewm_std = downside_returns.ewm(halflife=halflives).std()\n",
    "    annualized_ewm_std = ewm_std * np.sqrt(TRADING_DAYS)\n",
    "    return annualized_ewm_std\n",
    "\n",
    "# Initialize a dictionary to store weird columns for each stock\n",
    "training_weird_columns_dict = {stock: [] for stock in stock_list}\n",
    "testing_weird_columns_dict = {stock: [] for stock in stock_list}\n",
    "\n",
    "# Training\n",
    "for stock in stock_list:\n",
    "    for halflife in [5, 10, 21]:\n",
    "        downside_deviation = weighted_moving_downside_deviation_with_log(training_stock_df[stock][stock], halflife)\n",
    "        downside_deviation = downside_deviation.reindex(training_stock_df[stock].index) \n",
    "        training_feature_df[stock][f'{stock}_downside_deviation_{halflife}'] = downside_deviation\n",
    "\n",
    "        if check_weird_data(training_feature_df[stock][f'{stock}_downside_deviation_{halflife}']):\n",
    "            print(f\"Feature {stock}_downside_deviation_{halflife} contains weird data\")\n",
    "            training_weird_columns_dict[stock].append(f'{stock}_downside_deviation_{halflife}')\n",
    "\n",
    "# Testing\n",
    "for stock in stock_list:\n",
    "    for halflife in [5, 10, 21]:\n",
    "        downside_deviation = weighted_moving_downside_deviation_with_log(testing_stock_df[stock][stock], halflife)\n",
    "        downside_deviation = downside_deviation.reindex(testing_stock_df[stock].index) \n",
    "        testing_feature_df[stock][f'{stock}_downside_deviation_{halflife}'] = downside_deviation\n",
    "\n",
    "        if check_weird_data(testing_feature_df[stock][f'{stock}_downside_deviation_{halflife}']):\n",
    "            print(f\"Feature {stock}_downside_deviation_{halflife} contains weird data\")\n",
    "            testing_weird_columns_dict[stock].append(f'{stock}_downside_deviation_{halflife}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the weird data with KNN imputer\n",
    "def handle_weird_data_with_knn_imputer(data, weird_columns):\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    data[weird_columns] = imputer.fit_transform(data[weird_columns])\n",
    "    return data\n",
    "\n",
    "\n",
    "# Training\n",
    "for stock, weird_columns in training_weird_columns_dict.items():\n",
    "    if weird_columns:\n",
    "        training_feature_df[stock] = handle_weird_data_with_knn_imputer(training_feature_df[stock], weird_columns)\n",
    "\n",
    "# Testing\n",
    "for stock, weird_columns in testing_weird_columns_dict.items():\n",
    "    if weird_columns:\n",
    "        testing_feature_df[stock] = handle_weird_data_with_knn_imputer(testing_feature_df[stock], weird_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data again\n",
    "for stock, stock_data in training_feature_df.items():\n",
    "    if check_weird_data(stock_data):\n",
    "        print(f\"The {stock} stock price data still contains weird data in training data\")\n",
    "\n",
    "for stock, stock_data in testing_feature_df.items():\n",
    "    if check_weird_data(stock_data):\n",
    "        print(f\"The {stock} stock price data still contains weird data in testing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear weird_columns_dict, only clear the value, not the key\n",
    "for stock in stock_list:\n",
    "    training_weird_columns_dict[stock] = []\n",
    "\n",
    "for stock in stock_list:\n",
    "    testing_weird_columns_dict[stock] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500                   0.184606                    0.193321                    0.200102\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143                   0.184606                    0.193321                    0.200102\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531                   0.184606                    0.193321                    0.200102\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631                   0.184606                    0.193321                    0.200102\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759                   0.184606                    0.193321                    0.200102\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000                   0.110874                    0.116524                    0.117283\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644                   0.110874                    0.116524                    0.117283\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623                   0.110874                    0.116524                    0.117283\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726                   0.110874                    0.116524                    0.117283\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025                   0.110874                    0.116524                    0.117283\n"
     ]
    }
   ],
   "source": [
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 7 - 9: Sortino ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMZN daily return contains weird data in training data\n",
      "GOOGL daily return contains weird data in training data\n",
      "MSFT daily return contains weird data in training data\n",
      "NVDA daily return contains weird data in training data\n",
      "AAPL daily return contains weird data in training data\n",
      "NFLX daily return contains weird data in training data\n",
      "AVGO daily return contains weird data in training data\n",
      "TSLA daily return contains weird data in training data\n",
      "META daily return contains weird data in training data\n",
      "AMZN daily return contains weird data in testing data\n",
      "GOOGL daily return contains weird data in testing data\n",
      "MSFT daily return contains weird data in testing data\n",
      "NVDA daily return contains weird data in testing data\n",
      "AAPL daily return contains weird data in testing data\n",
      "NFLX daily return contains weird data in testing data\n",
      "AVGO daily return contains weird data in testing data\n",
      "TSLA daily return contains weird data in testing data\n",
      "META daily return contains weird data in testing data\n"
     ]
    }
   ],
   "source": [
    "# First, calculate the daily return of the stock\n",
    "def daily_return(stock_price):\n",
    "    return stock_price.pct_change()\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    stock_data[f'{stock}_daily_return'] = daily_return(stock_data[stock])\n",
    "    if check_weird_data(stock_data[f'{stock}_daily_return']):\n",
    "            print(f\"{stock} daily return contains weird data in training data\")\n",
    "            training_weird_columns_dict[stock].append(f'{stock}_daily_return')\n",
    "\n",
    "# Testing\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    stock_data[f'{stock}_daily_return'] = daily_return(stock_data[stock])\n",
    "    if check_weird_data(stock_data[f'{stock}_daily_return']):\n",
    "            print(f\"{stock} daily return contains weird data in testing data\")\n",
    "            testing_weird_columns_dict[stock].append(f'{stock}_daily_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the weird data with KNN imputer\n",
    "# Training\n",
    "for stock, weird_columns in training_weird_columns_dict.items():\n",
    "    if weird_columns:\n",
    "        training_stock_df[stock] = handle_weird_data_with_knn_imputer(training_stock_df[stock], weird_columns)\n",
    "\n",
    "# Testing\n",
    "for stock, weird_columns in testing_weird_columns_dict.items():\n",
    "    if weird_columns:\n",
    "        testing_stock_df[stock] = handle_weird_data_with_knn_imputer(testing_stock_df[stock], weird_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data again\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    if check_weird_data(stock_data):\n",
    "        print(f\"The {stock} stock price data still contains weird data in training data\")\n",
    "\n",
    "# Testing\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    if check_weird_data(stock_data):\n",
    "        print(f\"The {stock} stock price data still contains weird data in testing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear weird_columns_dict\n",
    "for stock in stock_list: \n",
    "    training_weird_columns_dict[stock] = []\n",
    "    testing_weird_columns_dict[stock] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calculating the Sortino ratio, \n",
    "# we need to append the daily risk free rate and \n",
    "# downside deviation to the stock data\n",
    "\n",
    "daily_risk_free_rate = pd.read_excel(f\"{DATA_DIR}/risk_free_rate/(Fama-French 3 Factors Plus Momentum - Daily Frequency)Risk-Free_Return_Rate.xlsx\")\n",
    "daily_risk_free_rate.rename(columns={'Risk-Free Return Rate (One Month Treasury Bill Rate)': 'daily_risk_free_rate'}, inplace=True)\n",
    "\n",
    "# Handle unmatched date format\n",
    "def parse_dates(date_series):\n",
    "    return pd.to_datetime(date_series, dayfirst=False)\n",
    "\n",
    "daily_risk_free_rate['date'] = parse_dates(daily_risk_free_rate['date'])\n",
    "\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    training_stock_df[stock] = pd.merge(stock_data, daily_risk_free_rate, on='date', how='inner')\n",
    "    for halflife in [5, 10, 21]:\n",
    "        downside_deviation = training_feature_df[stock][['date', f'{stock}_downside_deviation_{halflife}']]\n",
    "        training_stock_df[stock] = pd.merge(training_stock_df[stock], downside_deviation, on='date', how='inner')\n",
    "\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    testing_stock_df[stock] = pd.merge(stock_data, daily_risk_free_rate, on='date', how='inner')\n",
    "    for halflife in [5, 10, 21]:\n",
    "        downside_deviation = testing_feature_df[stock][['date', f'{stock}_downside_deviation_{halflife}']]\n",
    "        testing_stock_df[stock] = pd.merge(testing_stock_df[stock], downside_deviation, on='date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Sortino ratio\n",
    "def sortino_ratio(daily_return, risk_free_rate, downside_deviation):\n",
    "    return (daily_return - risk_free_rate) / downside_deviation\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    for halflife in [5, 10, 21]:\n",
    "        daily_return = stock_data[f'{stock}_daily_return']\n",
    "        risk_free_rate = stock_data['daily_risk_free_rate']\n",
    "        downside_deviation = stock_data[f'{stock}_downside_deviation_{halflife}']\n",
    "        training_feature_df[stock][f'{stock}_sortino_ratio_{halflife}'] = sortino_ratio(daily_return, risk_free_rate, downside_deviation)\n",
    "\n",
    "        if check_weird_data(training_feature_df[stock][f'{stock}_sortino_ratio_{halflife}']):\n",
    "            print(f\"{stock} sortino ratio {halflife} contains weird data in training data\")\n",
    "            training_weird_columns_dict[stock].append(f'{stock}_sortino_ratio_{halflife}')\n",
    "\n",
    "# Testing\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    for halflife in [5, 10, 21]:\n",
    "        daily_return = stock_data[f'{stock}_daily_return']\n",
    "        risk_free_rate = stock_data['daily_risk_free_rate']\n",
    "        downside_deviation = stock_data[f'{stock}_downside_deviation_{halflife}']\n",
    "        testing_feature_df[stock][f'{stock}_sortino_ratio_{halflife}'] = sortino_ratio(daily_return, risk_free_rate, downside_deviation)\n",
    "\n",
    "        if check_weird_data(testing_feature_df[stock][f'{stock}_sortino_ratio_{halflife}']):\n",
    "            print(f\"{stock} sortino ratio {halflife} contains weird data in testing data\")\n",
    "            testing_weird_columns_dict[stock].append(f'{stock}_sortino_ratio_{halflife}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500                   0.184606                    0.193321                    0.200102              0.006554               0.006258               0.006046\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143                   0.184606                    0.193321                    0.200102             -0.050896              -0.048602              -0.046955\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531                   0.184606                    0.193321                    0.200102              0.030740               0.029354               0.028360\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631                   0.184606                    0.193321                    0.200102              0.023195               0.022149               0.021398\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759                   0.184606                    0.193321                    0.200102              0.041727               0.039846               0.038496\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000                   0.110874                    0.116524                    0.117283              0.014814               0.014095               0.014004\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644                   0.110874                    0.116524                    0.117283              0.091493               0.087057               0.086494\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623                   0.110874                    0.116524                    0.117283             -0.097179              -0.092467              -0.091869\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726                   0.110874                    0.116524                    0.117283              0.330321               0.314304               0.312272\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025                   0.110874                    0.116524                    0.117283              0.035345               0.033631               0.033414\n"
     ]
    }
   ],
   "source": [
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date        AAPL  AAPL_daily_return  daily_risk_free_rate  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21\n",
      "0 2023-01-03  125.070000           0.001812               0.00017                   0.110874                    0.116524                    0.117283\n",
      "1 2023-01-04  126.360001           0.010314               0.00017                   0.110874                    0.116524                    0.117283\n",
      "2 2023-01-05  125.019997          -0.010605               0.00017                   0.110874                    0.116524                    0.117283\n",
      "3 2023-01-06  129.619995           0.036794               0.00017                   0.110874                    0.116524                    0.117283\n",
      "4 2023-01-09  130.149994           0.004089               0.00017                   0.110874                    0.116524                    0.117283\n",
      "        date       AAPL  AAPL_daily_return  daily_risk_free_rate  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21\n",
      "0 2016-07-01  23.972500           0.001220               0.00001                   0.184606                    0.193321                    0.200102\n",
      "1 2016-07-05  23.747499          -0.009386               0.00001                   0.184606                    0.193321                    0.200102\n",
      "2 2016-07-06  23.882500           0.005685               0.00001                   0.184606                    0.193321                    0.200102\n",
      "3 2016-07-07  23.985001           0.004292               0.00001                   0.184606                    0.193321                    0.200102\n",
      "4 2016-07-08  24.170000           0.007713               0.00001                   0.184606                    0.193321                    0.200102\n"
     ]
    }
   ],
   "source": [
    "print(testing_stock_df['AAPL'].head().to_string())\n",
    "print(training_stock_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature AMZN_mom12m contains weird data in training data\n",
      "Feature GOOGL_mom12m contains weird data in training data\n",
      "Feature MSFT_mom12m contains weird data in training data\n",
      "Feature NVDA_mom12m contains weird data in training data\n",
      "Feature AAPL_mom12m contains weird data in training data\n",
      "Feature NFLX_mom12m contains weird data in training data\n",
      "Feature AVGO_mom12m contains weird data in training data\n",
      "Feature TSLA_mom12m contains weird data in training data\n",
      "Feature META_mom12m contains weird data in training data\n"
     ]
    }
   ],
   "source": [
    "def momentum_12m(stock_price):\n",
    "    price_1m_ago = stock_price.shift(21)\n",
    "    price_12m_ago = stock_price.shift(252)\n",
    "    return price_1m_ago / price_12m_ago - 1\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    training_feature_df[stock][f'{stock}_mom12m'] = momentum_12m(stock_data[stock])\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_mom12m']):\n",
    "        print(f\"Feature {stock}_mom12m contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_mom12m')\n",
    "\n",
    "# Notice that the testing period is too short to calculate the 12-month momentum\n",
    "# So it will be calculated in later stage if this feature is selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for stock, weird_columns in training_weird_columns_dict.items():\n",
    "    if weird_columns:\n",
    "        training_feature_df[stock] = handle_weird_data_with_knn_imputer(training_feature_df[stock], weird_columns)\n",
    "\n",
    "# Check the data again\n",
    "for stock, stock_data in training_feature_df.items():\n",
    "    if check_weird_data(stock_data):\n",
    "        print(f\"The {stock} stock price data still contains weird data in training data\")\n",
    "\n",
    "# Clear weird_columns_dict\n",
    "for stock in stock_list:\n",
    "    training_weird_columns_dict[stock] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 11: short-term reversal (mom1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature AMZN_mom1m contains weird data in training data\n",
      "Feature GOOGL_mom1m contains weird data in training data\n",
      "Feature MSFT_mom1m contains weird data in training data\n",
      "Feature NVDA_mom1m contains weird data in training data\n",
      "Feature AAPL_mom1m contains weird data in training data\n",
      "Feature NFLX_mom1m contains weird data in training data\n",
      "Feature AVGO_mom1m contains weird data in training data\n",
      "Feature TSLA_mom1m contains weird data in training data\n",
      "Feature META_mom1m contains weird data in training data\n",
      "Feature AMZN_mom1m contains weird data in testing data\n",
      "Feature GOOGL_mom1m contains weird data in testing data\n",
      "Feature MSFT_mom1m contains weird data in testing data\n",
      "Feature NVDA_mom1m contains weird data in testing data\n",
      "Feature AAPL_mom1m contains weird data in testing data\n",
      "Feature NFLX_mom1m contains weird data in testing data\n",
      "Feature AVGO_mom1m contains weird data in testing data\n",
      "Feature TSLA_mom1m contains weird data in testing data\n",
      "Feature META_mom1m contains weird data in testing data\n"
     ]
    }
   ],
   "source": [
    "def momentum_1m(stock_price):\n",
    "    stock_price_1m_ago = stock_price.shift(21)\n",
    "    return stock_price / stock_price_1m_ago - 1\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    training_feature_df[stock][f'{stock}_mom1m'] = momentum_1m(stock_data[stock])\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_mom1m']):\n",
    "        print(f\"Feature {stock}_mom1m contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_mom1m')\n",
    "\n",
    "# Testing\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    testing_feature_df[stock][f'{stock}_mom1m'] = momentum_1m(stock_data[stock])\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_mom1m']):\n",
    "        print(f\"Feature {stock}_mom1m contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_mom1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for stock, weird_columns in training_weird_columns_dict.items():\n",
    "    if weird_columns:\n",
    "        training_feature_df[stock] = handle_weird_data_with_knn_imputer(training_feature_df[stock], weird_columns)\n",
    "\n",
    "# Testing\n",
    "for stock, weird_columns in testing_weird_columns_dict.items():\n",
    "    if weird_columns:\n",
    "        testing_feature_df[stock] = handle_weird_data_with_knn_imputer(testing_feature_df[stock], weird_columns)\n",
    "\n",
    "# Check the data again\n",
    "for stock, stock_data in training_feature_df.items():\n",
    "    if check_weird_data(stock_data):\n",
    "        print(f\"The {stock} stock price data still contains weird data in training data\")\n",
    "\n",
    "for stock, stock_data in testing_feature_df.items():\n",
    "    if check_weird_data(stock_data):\n",
    "        print(f\"The {stock} stock price data still contains weird data in testing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear weird_columns_dict, only clear the value, not the key\n",
    "for stock in stock_list:\n",
    "    training_weird_columns_dict[stock] = []\n",
    "\n",
    "for stock in stock_list:\n",
    "    testing_weird_columns_dict[stock] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom12m  AAPL_mom1m\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500                   0.184606                    0.193321                    0.200102              0.006554               0.006258               0.006046     0.347234    0.026148\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143                   0.184606                    0.193321                    0.200102             -0.050896              -0.048602              -0.046955     0.347234    0.026148\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531                   0.184606                    0.193321                    0.200102              0.030740               0.029354               0.028360     0.347234    0.026148\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631                   0.184606                    0.193321                    0.200102              0.023195               0.022149               0.021398     0.347234    0.026148\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759                   0.184606                    0.193321                    0.200102              0.041727               0.039846               0.038496     0.347234    0.026148\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom1m\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000                   0.110874                    0.116524                    0.117283              0.014814               0.014095               0.014004    0.034747\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644                   0.110874                    0.116524                    0.117283              0.091493               0.087057               0.086494    0.034747\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623                   0.110874                    0.116524                    0.117283             -0.097179              -0.092467              -0.091869    0.034747\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726                   0.110874                    0.116524                    0.117283              0.330321               0.314304               0.312272    0.034747\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025                   0.110874                    0.116524                    0.117283              0.035345               0.033631               0.033414    0.034747\n"
     ]
    }
   ],
   "source": [
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 12: 6-month momentum (mom6m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature AMZN_mom6m contains weird data\n",
      "Feature GOOGL_mom6m contains weird data\n",
      "Feature MSFT_mom6m contains weird data\n",
      "Feature NVDA_mom6m contains weird data\n",
      "Feature AAPL_mom6m contains weird data\n",
      "Feature NFLX_mom6m contains weird data\n",
      "Feature AVGO_mom6m contains weird data\n",
      "Feature TSLA_mom6m contains weird data\n",
      "Feature META_mom6m contains weird data\n",
      "Feature AMZN_mom6m contains weird data\n",
      "Feature GOOGL_mom6m contains weird data\n",
      "Feature MSFT_mom6m contains weird data\n",
      "Feature NVDA_mom6m contains weird data\n",
      "Feature AAPL_mom6m contains weird data\n",
      "Feature NFLX_mom6m contains weird data\n",
      "Feature AVGO_mom6m contains weird data\n",
      "Feature TSLA_mom6m contains weird data\n",
      "Feature META_mom6m contains weird data\n"
     ]
    }
   ],
   "source": [
    "def momentum_6m(stock_price):\n",
    "    stock_price_6m_ago = stock_price.shift(126)\n",
    "    return stock_price / stock_price_6m_ago - 1\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    training_feature_df[stock][f'{stock}_mom6m'] = momentum_6m(stock_data[stock])\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_mom6m']):\n",
    "        print(f\"Feature {stock}_mom6m contains weird data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_mom6m')\n",
    "\n",
    "# Testing\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    testing_feature_df[stock][f'{stock}_mom6m'] = momentum_6m(stock_data[stock])\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_mom6m']):\n",
    "        print(f\"Feature {stock}_mom6m contains weird data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_mom6m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for stock, weird_columns in training_weird_columns_dict.items():\n",
    "    if weird_columns:\n",
    "        training_feature_df[stock] = handle_weird_data_with_knn_imputer(training_feature_df[stock], weird_columns)\n",
    "\n",
    "# Testing\n",
    "for stock, weird_columns in testing_weird_columns_dict.items():\n",
    "    if weird_columns:\n",
    "        testing_feature_df[stock] = handle_weird_data_with_knn_imputer(testing_feature_df[stock], weird_columns)\n",
    "\n",
    "# Check the data again\n",
    "for stock, stock_data in training_feature_df.items():\n",
    "    if check_weird_data(stock_data):\n",
    "        print(f\"The {stock} stock price data still contains weird data in training data\")\n",
    "\n",
    "for stock, stock_data in testing_feature_df.items():\n",
    "    if check_weird_data(stock_data):\n",
    "        print(f\"The {stock} stock price data still contains weird data in testing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom12m  AAPL_mom1m  AAPL_mom6m\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500                   0.184606                    0.193321                    0.200102              0.006554               0.006258               0.006046     0.347234    0.026148    0.169584\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143                   0.184606                    0.193321                    0.200102             -0.050896              -0.048602              -0.046955     0.347234    0.026148    0.169584\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531                   0.184606                    0.193321                    0.200102              0.030740               0.029354               0.028360     0.347234    0.026148    0.169584\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631                   0.184606                    0.193321                    0.200102              0.023195               0.022149               0.021398     0.347234    0.026148    0.169584\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759                   0.184606                    0.193321                    0.200102              0.041727               0.039846               0.038496     0.347234    0.026148    0.169584\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom1m  AAPL_mom6m\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000                   0.110874                    0.116524                    0.117283              0.014814               0.014095               0.014004    0.034747    0.154754\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644                   0.110874                    0.116524                    0.117283              0.091493               0.087057               0.086494    0.034747    0.154754\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623                   0.110874                    0.116524                    0.117283             -0.097179              -0.092467              -0.091869    0.034747    0.154754\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726                   0.110874                    0.116524                    0.117283              0.330321               0.314304               0.312272    0.034747    0.154754\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025                   0.110874                    0.116524                    0.117283              0.035345               0.033631               0.033414    0.034747    0.154754\n"
     ]
    }
   ],
   "source": [
    "# Clear weird_columns_dict, only clear the value, not the key\n",
    "for stock in stock_list:\n",
    "    training_weird_columns_dict[stock] = []\n",
    "\n",
    "for stock in stock_list:\n",
    "    testing_weird_columns_dict[stock] = []\n",
    "\n",
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 14: Log Market Equity (mvel1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Notice that we already have the daily price data,\n",
    "and I found shareoutstanding data can be found on wrds, shrout is the number of shares outstanding\n",
    "https://wrds-www.wharton.upenn.edu/pages/get-data/center-research-security-prices-crsp/annual-update/stock-events/share-outstanding/\n",
    "PERMNO is the unique identifier for each stock\n",
    "AMZN: 84788\n",
    "GOOGL: 90319\n",
    "MSFT: 10107\n",
    "NVDA: 86580\n",
    "AAPL: 14593\n",
    "NFLX: 89393\n",
    "AVGO: 93002\n",
    "TSLA: 93436\n",
    "META: 13407\n",
    "'''\n",
    "permno_dict = config_data['permno_dict']\n",
    "def share_outstanding_preprocessing():\n",
    "    # Load the share outstanding data\n",
    "    share_outstanding_data = pd.read_excel(f\"{DATA_DIR}/company_fundamentals/{stock}_share_outstanding_raw.xlsx\")\n",
    "\n",
    "    # Make sure the downloaded data is correct\n",
    "    if int(permno_dict[stock]) != int(share_outstanding_data['PERMNO'][0]):\n",
    "        print(\"You downloaded the wrong data for {stock}\")\n",
    "        exit()\n",
    "\n",
    "    # rename the column from Shares Observation Date to date\n",
    "    share_outstanding_data = share_outstanding_data.rename(columns={'Shares Observation Date': 'date'})\n",
    "    # Convert the date column to datetime\n",
    "    share_outstanding_data['date'] =  parse_dates(share_outstanding_data['date'])\n",
    "    # Drop PERMNO column\n",
    "    share_outstanding_data = share_outstanding_data.drop(columns=['PERMNO'])\n",
    "    # Notice that the original data is expressed in thousand, so need to multiple the data by 1000\n",
    "    share_outstanding_data['Shares Outstanding'] *= 1000\n",
    "\n",
    "    return share_outstanding_data\n",
    "\n",
    "def mvel1(stock_price, share_outstanding):\n",
    "    return np.log(stock_price * share_outstanding + 1e-9)\n",
    "\n",
    "# Share outstanding data (Training)\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    share_outstanding_data = share_outstanding_preprocessing()\n",
    "    # Merge the share outstanding data with the stock price data (Training)\n",
    "    training_date = parse_dates(training_stock_df[stock]['date'])\n",
    "    training_share_outstanding = pd.merge(training_date, share_outstanding_data, on='date', how='outer')\n",
    "\n",
    "    # Backfill the share outstanding data\n",
    "    training_share_outstanding = training_share_outstanding.bfill()\n",
    "\n",
    "    training_feature_df[stock][f'{stock}_mvel1'] = mvel1(stock_data[stock], training_share_outstanding['Shares Outstanding'])\n",
    "\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_mvel1']):\n",
    "        print(f\"Feature {stock}_mvel1 contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_mvel1')\n",
    "\n",
    "# Share outstanding data (Testing)\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    # Merge the share outstanding data with the stock price data (Testing)\n",
    "    testing_date = parse_dates(testing_stock_df[stock]['date'])\n",
    "    testing_share_outstanding = pd.merge(testing_date, share_outstanding_data, on='date', how='outer')\n",
    "\n",
    "    # Backfill the share outstanding data\n",
    "    testing_share_outstanding = testing_share_outstanding.bfill()\n",
    "\n",
    "    # There are still some missing data in the share outstanding data\n",
    "    # So we need to forward fill the data\n",
    "    testing_share_outstanding = testing_share_outstanding.ffill()\n",
    "\n",
    "    testing_feature_df[stock][f'{stock}_mvel1'] = mvel1(stock_data[stock], testing_share_outstanding['Shares Outstanding'])\n",
    "\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_mvel1']):\n",
    "        print(f\"Feature {stock}_mvel1 contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_mvel1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom12m  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500                   0.184606                    0.193321                    0.200102              0.006554               0.006258               0.006046     0.347234    0.026148    0.169584   23.804209\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143                   0.184606                    0.193321                    0.200102             -0.050896              -0.048602              -0.046955     0.347234    0.026148    0.169584   23.784508\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531                   0.184606                    0.193321                    0.200102              0.030740               0.029354               0.028360     0.347234    0.026148    0.169584   23.790760\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631                   0.184606                    0.193321                    0.200102              0.023195               0.022149               0.021398     0.347234    0.026148    0.169584   23.787023\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759                   0.184606                    0.193321                    0.200102              0.041727               0.039846               0.038496     0.347234    0.026148    0.169584   23.794590\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000                   0.110874                    0.116524                    0.117283              0.014814               0.014095               0.014004    0.034747    0.154754   26.149610\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644                   0.110874                    0.116524                    0.117283              0.091493               0.087057               0.086494    0.034747    0.154754   26.187804\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623                   0.110874                    0.116524                    0.117283             -0.097179              -0.092467              -0.091869    0.034747    0.154754   26.179840\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726                   0.110874                    0.116524                    0.117283              0.330321               0.314304               0.312272    0.034747    0.154754   26.265743\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025                   0.110874                    0.116524                    0.117283              0.035345               0.033631               0.033414    0.034747    0.154754   26.269987\n"
     ]
    }
   ],
   "source": [
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 15: Standard deviation of daily returns from month t-1 (retvol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Source: https://wrds-www.wharton.upenn.edu/pages/get-data/contributed-data-forms/global-factor-data/\n",
    "PERMNO is the unique identifier for each stock\n",
    "Amazon: 84788\n",
    "Google: 90319\n",
    "Microsoft: 10107\n",
    "Nvidia: 86580\n",
    "Apple: 14593\n",
    "NFLX: 89393\n",
    "AVGO: 93002\n",
    "Tesla: 93436\n",
    "Meta: 13407\n",
    "\"\"\"\n",
    "def get_rvol_21d(stock):\n",
    "    global_factor = pd.read_excel(f\"{DATA_DIR}/global_factor/{stock}_global_factor.xlsx\")\n",
    "    target_col = ['Day of last price observation (date)', 'Return volatility (rvol_21d)']\n",
    "    rvol_21d = global_factor[target_col].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Rename the columns\n",
    "    rvol_21d.rename(columns={'Return volatility (rvol_21d)': f'{stock}_rvol_21d'}, inplace=True)\n",
    "\n",
    "    # Parse the date column\n",
    "    rvol_21d['Day of last price observation (date)'] = parse_dates(rvol_21d['Day of last price observation (date)'])\n",
    "\n",
    "    return rvol_21d\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    rvol_21d = get_rvol_21d(stock)\n",
    "\n",
    "    # Merge the rvol_21d data with the stock data\n",
    "    training_feature_df[stock] = pd.merge(training_feature_df[stock], rvol_21d, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    training_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values \n",
    "    training_feature_df[stock][f'{stock}_rvol_21d'] = training_feature_df[stock][f'{stock}_rvol_21d'].bfill()\n",
    "\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_rvol_21d']):\n",
    "        print(f\"Feature f'{stock}_rvol_21d' contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_rvol_21d')\n",
    "\n",
    "# Testing\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    rvol_21d = get_rvol_21d(stock)\n",
    "\n",
    "    # Merge the rvol_21d data with the stock data\n",
    "    testing_feature_df[stock] = pd.merge(testing_feature_df[stock], rvol_21d, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    testing_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    testing_feature_df[stock][f'{stock}_rvol_21d'] = testing_feature_df[stock][f'{stock}_rvol_21d'].bfill()\n",
    "\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_rvol_21d']):\n",
    "        print(f\"Feature f'{stock}_rvol_21d' contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_rvol_21d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom12m  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500                   0.184606                    0.193321                    0.200102              0.006554               0.006258               0.006046     0.347234    0.026148    0.169584   23.804209       0.016383\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143                   0.184606                    0.193321                    0.200102             -0.050896              -0.048602              -0.046955     0.347234    0.026148    0.169584   23.784508       0.016383\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531                   0.184606                    0.193321                    0.200102              0.030740               0.029354               0.028360     0.347234    0.026148    0.169584   23.790760       0.016383\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631                   0.184606                    0.193321                    0.200102              0.023195               0.022149               0.021398     0.347234    0.026148    0.169584   23.787023       0.016383\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759                   0.184606                    0.193321                    0.200102              0.041727               0.039846               0.038496     0.347234    0.026148    0.169584   23.794590       0.016383\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000                   0.110874                    0.116524                    0.117283              0.014814               0.014095               0.014004    0.034747    0.154754   26.149610       0.016254\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644                   0.110874                    0.116524                    0.117283              0.091493               0.087057               0.086494    0.034747    0.154754   26.187804       0.016254\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623                   0.110874                    0.116524                    0.117283             -0.097179              -0.092467              -0.091869    0.034747    0.154754   26.179840       0.016254\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726                   0.110874                    0.116524                    0.117283              0.330321               0.314304               0.312272    0.034747    0.154754   26.265743       0.016254\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025                   0.110874                    0.116524                    0.117283              0.035345               0.033631               0.033414    0.034747    0.154754   26.269987       0.016254\n"
     ]
    }
   ],
   "source": [
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 16-18: Change in Shares - 1 Month (chcsho_1m, chcsho_3m, chcsho_6m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chcsho(stock):\n",
    "    global_factor = pd.read_excel(f\"{DATA_DIR}/global_factor/{stock}_global_factor.xlsx\")\n",
    "    target_cols = ['Day of last price observation (date)', 'Change in Shares - 1 Month (chcsho_1m)', 'Change in Shares - 3 Month (chcsho_3m)', 'Change in Shares - 6 Month (chcsho_6m)']\n",
    "    chcsho = global_factor[target_cols].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Rename the columns\n",
    "    chcsho.rename(columns={'Change in Shares - 1 Month (chcsho_1m)': f'{stock}_chcsho_1m', 'Change in Shares - 3 Month (chcsho_3m)': f'{stock}_chcsho_3m', 'Change in Shares - 6 Month (chcsho_6m)': f'{stock}_chcsho_6m'}, inplace=True)\n",
    "\n",
    "    # Parse the date column\n",
    "    chcsho['Day of last price observation (date)'] = parse_dates(chcsho['Day of last price observation (date)'])\n",
    "\n",
    "    return chcsho\n",
    "\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    chcsho = get_chcsho(stock)\n",
    "\n",
    "    # Merge the chcsho data with the stock data\n",
    "    training_feature_df[stock] = pd.merge(training_feature_df[stock], chcsho, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    training_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    training_feature_df[stock][f'{stock}_chcsho_1m'] = training_feature_df[stock][f'{stock}_chcsho_1m'].bfill()\n",
    "    training_feature_df[stock][f'{stock}_chcsho_3m'] = training_feature_df[stock][f'{stock}_chcsho_3m'].bfill()\n",
    "    training_feature_df[stock][f'{stock}_chcsho_6m'] = training_feature_df[stock][f'{stock}_chcsho_6m'].bfill()\n",
    "\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_chcsho_1m']):\n",
    "        print(f\"Feature f'{stock}_chcsho_1m' contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_chcsho_1m')\n",
    "\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_chcsho_3m']):\n",
    "        print(f\"Feature f'{stock}_chcsho_3m' contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_chcsho_3m')\n",
    "\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_chcsho_6m']):\n",
    "        print(f\"Feature f'{stock}_chcsho_6m' contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_chcsho_6m')\n",
    "\n",
    "\n",
    "# Testing\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    chcsho = get_chcsho(stock)\n",
    "\n",
    "    # Merge the chcsho data with the stock data\n",
    "    testing_feature_df[stock] = pd.merge(testing_feature_df[stock], chcsho, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    testing_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    testing_feature_df[stock][f'{stock}_chcsho_1m'] = testing_feature_df[stock][f'{stock}_chcsho_1m'].bfill()\n",
    "    testing_feature_df[stock][f'{stock}_chcsho_3m'] = testing_feature_df[stock][f'{stock}_chcsho_3m'].bfill()\n",
    "    testing_feature_df[stock][f'{stock}_chcsho_6m'] = testing_feature_df[stock][f'{stock}_chcsho_6m'].bfill()\n",
    "\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_chcsho_1m']):\n",
    "        print(f\"Feature f'{stock}_chcsho_1m' contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_chcsho_1m')\n",
    "\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_chcsho_3m']):\n",
    "        print(f\"Feature f'{stock}_chcsho_3m' contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_chcsho_3m')\n",
    "\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_chcsho_6m']):\n",
    "        print(f\"Feature f'{stock}_chcsho_6m' contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_chcsho_6m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom12m  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500                   0.184606                    0.193321                    0.200102              0.006554               0.006258               0.006046     0.347234    0.026148    0.169584   23.804209       0.016383       -0.000876       -0.016245       -0.028161\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143                   0.184606                    0.193321                    0.200102             -0.050896              -0.048602              -0.046955     0.347234    0.026148    0.169584   23.784508       0.016383       -0.000876       -0.016245       -0.028161\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531                   0.184606                    0.193321                    0.200102              0.030740               0.029354               0.028360     0.347234    0.026148    0.169584   23.790760       0.016383       -0.000876       -0.016245       -0.028161\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631                   0.184606                    0.193321                    0.200102              0.023195               0.022149               0.021398     0.347234    0.026148    0.169584   23.787023       0.016383       -0.000876       -0.016245       -0.028161\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759                   0.184606                    0.193321                    0.200102              0.041727               0.039846               0.038496     0.347234    0.026148    0.169584   23.794590       0.016383       -0.000876       -0.016245       -0.028161\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000                   0.110874                    0.116524                    0.117283              0.014814               0.014095               0.014004    0.034747    0.154754   26.149610       0.016254       -0.001292       -0.005417       -0.015482\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644                   0.110874                    0.116524                    0.117283              0.091493               0.087057               0.086494    0.034747    0.154754   26.187804       0.016254       -0.001292       -0.005417       -0.015482\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623                   0.110874                    0.116524                    0.117283             -0.097179              -0.092467              -0.091869    0.034747    0.154754   26.179840       0.016254       -0.001292       -0.005417       -0.015482\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726                   0.110874                    0.116524                    0.117283              0.330321               0.314304               0.312272    0.034747    0.154754   26.265743       0.016254       -0.001292       -0.005417       -0.015482\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025                   0.110874                    0.116524                    0.117283              0.035345               0.033631               0.033414    0.034747    0.154754   26.269987       0.016254       -0.001292       -0.005417       -0.015482\n"
     ]
    }
   ],
   "source": [
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 19: Amihud illiquidity (ami_126d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ami_126d(stock):\n",
    "    global_factor = pd.read_excel(f\"{DATA_DIR}/global_factor/{stock}_global_factor.xlsx\")\n",
    "    target_cols = ['Day of last price observation (date)', 'Amihud Measure (ami_126d)']\n",
    "    ami_126d = global_factor[target_cols].copy()\n",
    "    \n",
    "\n",
    "    # Rename the columns\n",
    "    ami_126d.rename(columns={'Amihud Measure (ami_126d)': f'{stock}_ami_126d'}, inplace=True)\n",
    "\n",
    "    # Parse the date column\n",
    "    ami_126d['Day of last price observation (date)'] = parse_dates(ami_126d['Day of last price observation (date)'])\n",
    "\n",
    "    return ami_126d\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    ami_126d = get_ami_126d(stock)\n",
    "\n",
    "    # Merge the ami_126d data with the stock data\n",
    "    training_feature_df[stock] = pd.merge(training_feature_df[stock], ami_126d, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    training_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    training_feature_df[stock][f'{stock}_ami_126d'] = training_feature_df[stock][f'{stock}_ami_126d'].bfill()\n",
    "\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_ami_126d']):\n",
    "        print(f\"Feature f'{stock}_ami_126d' contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_ami_126d')\n",
    "\n",
    "# Testing\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    ami_126d = get_ami_126d(stock)\n",
    "\n",
    "    # Merge the ami_126d data with the stock data\n",
    "    testing_feature_df[stock] = pd.merge(testing_feature_df[stock], ami_126d, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    testing_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    testing_feature_df[stock][f'{stock}_ami_126d'] = testing_feature_df[stock][f'{stock}_ami_126d'].bfill()\n",
    "\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_ami_126d']):\n",
    "        print(f\"Feature f'{stock}_ami_126d' contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_ami_126d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom12m  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m  AAPL_ami_126d\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500                   0.184606                    0.193321                    0.200102              0.006554               0.006258               0.006046     0.347234    0.026148    0.169584   23.804209       0.016383       -0.000876       -0.016245       -0.028161       0.000003\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143                   0.184606                    0.193321                    0.200102             -0.050896              -0.048602              -0.046955     0.347234    0.026148    0.169584   23.784508       0.016383       -0.000876       -0.016245       -0.028161       0.000003\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531                   0.184606                    0.193321                    0.200102              0.030740               0.029354               0.028360     0.347234    0.026148    0.169584   23.790760       0.016383       -0.000876       -0.016245       -0.028161       0.000003\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631                   0.184606                    0.193321                    0.200102              0.023195               0.022149               0.021398     0.347234    0.026148    0.169584   23.787023       0.016383       -0.000876       -0.016245       -0.028161       0.000003\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759                   0.184606                    0.193321                    0.200102              0.041727               0.039846               0.038496     0.347234    0.026148    0.169584   23.794590       0.016383       -0.000876       -0.016245       -0.028161       0.000003\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m  AAPL_ami_126d\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000                   0.110874                    0.116524                    0.117283              0.014814               0.014095               0.014004    0.034747    0.154754   26.149610       0.016254       -0.001292       -0.005417       -0.015482       0.000001\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644                   0.110874                    0.116524                    0.117283              0.091493               0.087057               0.086494    0.034747    0.154754   26.187804       0.016254       -0.001292       -0.005417       -0.015482       0.000001\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623                   0.110874                    0.116524                    0.117283             -0.097179              -0.092467              -0.091869    0.034747    0.154754   26.179840       0.016254       -0.001292       -0.005417       -0.015482       0.000001\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726                   0.110874                    0.116524                    0.117283              0.330321               0.314304               0.312272    0.034747    0.154754   26.265743       0.016254       -0.001292       -0.005417       -0.015482       0.000001\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025                   0.110874                    0.116524                    0.117283              0.035345               0.033631               0.033414    0.034747    0.154754   26.269987       0.016254       -0.001292       -0.005417       -0.015482       0.000001\n"
     ]
    }
   ],
   "source": [
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 20: Age of the firms in months (age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age(stock):\n",
    "    global_factor = pd.read_excel(f\"{DATA_DIR}/global_factor/{stock}_global_factor.xlsx\")\n",
    "    target_cols = ['Day of last price observation (date)', 'Firm age (age)']\n",
    "    age = global_factor[target_cols].copy()\n",
    "    \n",
    "\n",
    "    # Rename the columns\n",
    "    age.rename(columns={'Firm age (age)': f'{stock}_age'}, inplace=True)\n",
    "\n",
    "    # Parse the date column\n",
    "    age['Day of last price observation (date)'] = parse_dates(age['Day of last price observation (date)'])\n",
    "\n",
    "    return age\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    age = get_age(stock)\n",
    "\n",
    "    # Merge the age data with the stock data\n",
    "    training_feature_df[stock] = pd.merge(training_feature_df[stock], age, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    training_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    training_feature_df[stock][f'{stock}_age'] = training_feature_df[stock][f'{stock}_age'].bfill()\n",
    "\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_age']):\n",
    "        print(f\"Feature f'{stock}_age' contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_age')\n",
    "\n",
    "# Testing\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    age = get_age(stock)\n",
    "\n",
    "    # Merge the age data with the stock data\n",
    "    testing_feature_df[stock] = pd.merge(testing_feature_df[stock], age, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    testing_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    testing_feature_df[stock][f'{stock}_age'] = testing_feature_df[stock][f'{stock}_age'].bfill()\n",
    "\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_age']):\n",
    "        print(f\"Feature f'{stock}_age' contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom12m  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m  AAPL_ami_126d  AAPL_age\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500                   0.184606                    0.193321                    0.200102              0.006554               0.006258               0.006046     0.347234    0.026148    0.169584   23.804209       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143                   0.184606                    0.193321                    0.200102             -0.050896              -0.048602              -0.046955     0.347234    0.026148    0.169584   23.784508       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531                   0.184606                    0.193321                    0.200102              0.030740               0.029354               0.028360     0.347234    0.026148    0.169584   23.790760       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631                   0.184606                    0.193321                    0.200102              0.023195               0.022149               0.021398     0.347234    0.026148    0.169584   23.787023       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759                   0.184606                    0.193321                    0.200102              0.041727               0.039846               0.038496     0.347234    0.026148    0.169584   23.794590       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m  AAPL_ami_126d  AAPL_age\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000                   0.110874                    0.116524                    0.117283              0.014814               0.014095               0.014004    0.034747    0.154754   26.149610       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644                   0.110874                    0.116524                    0.117283              0.091493               0.087057               0.086494    0.034747    0.154754   26.187804       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623                   0.110874                    0.116524                    0.117283             -0.097179              -0.092467              -0.091869    0.034747    0.154754   26.179840       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726                   0.110874                    0.116524                    0.117283              0.330321               0.314304               0.312272    0.034747    0.154754   26.265743       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025                   0.110874                    0.116524                    0.117283              0.035345               0.033631               0.033414    0.034747    0.154754   26.269987       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0\n"
     ]
    }
   ],
   "source": [
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 21: Change in 6-month momentum (chmom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature AMZN_chmom contains weird data in training data\n",
      "Feature GOOGL_chmom contains weird data in training data\n",
      "Feature MSFT_chmom contains weird data in training data\n",
      "Feature NVDA_chmom contains weird data in training data\n",
      "Feature AAPL_chmom contains weird data in training data\n",
      "Feature NFLX_chmom contains weird data in training data\n",
      "Feature AVGO_chmom contains weird data in training data\n",
      "Feature TSLA_chmom contains weird data in training data\n",
      "Feature META_chmom contains weird data in training data\n"
     ]
    }
   ],
   "source": [
    "def get_chmom(stock_price):\n",
    "    price_1m_ago = stock_price.shift(21)\n",
    "    price_6m_ago = stock_price.shift(126)\n",
    "\n",
    "    price_7m_ago = stock_price.shift(147)\n",
    "    price_12m_ago = stock_price.shift(252)\n",
    "    return (price_1m_ago / price_6m_ago) - 1 - ( (price_7m_ago / price_12m_ago) - 1 )\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    training_feature_df[stock][f'{stock}_chmom'] = get_chmom(stock_data[stock])\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_chmom']):\n",
    "        print(f\"Feature {stock}_chmom contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_chmom')\n",
    "\n",
    "# Similarly, the testing period is too short to calculate the change in 6-month momentum\n",
    "# So it will be calculated in later stage if this feature is selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for stock, weird_columns in training_weird_columns_dict.items():\n",
    "    if weird_columns:\n",
    "        training_feature_df[stock] = handle_weird_data_with_knn_imputer(training_feature_df[stock], weird_columns)\n",
    "\n",
    "# Check the data again\n",
    "for stock, stock_data in training_feature_df.items():\n",
    "    if check_weird_data(stock_data):\n",
    "        print(f\"The {stock} stock price data still contains weird data in training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom12m  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m  AAPL_ami_126d  AAPL_age  AAPL_chmom\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500                   0.184606                    0.193321                    0.200102              0.006554               0.006258               0.006046     0.347234    0.026148    0.169584   23.804209       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143                   0.184606                    0.193321                    0.200102             -0.050896              -0.048602              -0.046955     0.347234    0.026148    0.169584   23.784508       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531                   0.184606                    0.193321                    0.200102              0.030740               0.029354               0.028360     0.347234    0.026148    0.169584   23.790760       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631                   0.184606                    0.193321                    0.200102              0.023195               0.022149               0.021398     0.347234    0.026148    0.169584   23.787023       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759                   0.184606                    0.193321                    0.200102              0.041727               0.039846               0.038496     0.347234    0.026148    0.169584   23.794590       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m  AAPL_ami_126d  AAPL_age\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000                   0.110874                    0.116524                    0.117283              0.014814               0.014095               0.014004    0.034747    0.154754   26.149610       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644                   0.110874                    0.116524                    0.117283              0.091493               0.087057               0.086494    0.034747    0.154754   26.187804       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623                   0.110874                    0.116524                    0.117283             -0.097179              -0.092467              -0.091869    0.034747    0.154754   26.179840       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726                   0.110874                    0.116524                    0.117283              0.330321               0.314304               0.312272    0.034747    0.154754   26.265743       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025                   0.110874                    0.116524                    0.117283              0.035345               0.033631               0.033414    0.034747    0.154754   26.269987       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0\n"
     ]
    }
   ],
   "source": [
    "# Clear weird_columns_dict, only clear the value, not the key\n",
    "for stock in stock_list:\n",
    "    training_weird_columns_dict[stock] = []\n",
    "\n",
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 22: Maximum daily return (maxret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature AMZN_maxret contains weird data in training data\n",
      "Feature GOOGL_maxret contains weird data in training data\n",
      "Feature MSFT_maxret contains weird data in training data\n",
      "Feature NVDA_maxret contains weird data in training data\n",
      "Feature AAPL_maxret contains weird data in training data\n",
      "Feature NFLX_maxret contains weird data in training data\n",
      "Feature AVGO_maxret contains weird data in training data\n",
      "Feature TSLA_maxret contains weird data in training data\n",
      "Feature META_maxret contains weird data in training data\n",
      "Feature AMZN_maxret contains weird data in testing data\n",
      "Feature GOOGL_maxret contains weird data in testing data\n",
      "Feature MSFT_maxret contains weird data in testing data\n",
      "Feature NVDA_maxret contains weird data in testing data\n",
      "Feature AAPL_maxret contains weird data in testing data\n",
      "Feature NFLX_maxret contains weird data in testing data\n",
      "Feature AVGO_maxret contains weird data in testing data\n",
      "Feature TSLA_maxret contains weird data in testing data\n",
      "Feature META_maxret contains weird data in testing data\n"
     ]
    }
   ],
   "source": [
    "def get_maxret(stock_df, stock_ticket):\n",
    "    # Create a copy to avoid SettingWithCopy issues\n",
    "    df = stock_df.copy()\n",
    "    df.loc[:, 'year_month'] = df['date'].dt.to_period('M')\n",
    "    monthly_max = df.groupby('year_month')[f'{stock_ticket}_daily_return'].max().reset_index()\n",
    "    # Rename the computed column\n",
    "    monthly_max = monthly_max.rename(columns={f'{stock_ticket}_daily_return': f'{stock_ticket}_maxret'})\n",
    "    # Shift the max returns down by one period so that each row corresponds to calendar month t-1\n",
    "    monthly_max[f'{stock_ticket}_maxret'] = monthly_max[f'{stock_ticket}_maxret'].shift(1)\n",
    "    # Convert the period back to a timestamp; here we choose the month-end timestamp\n",
    "    monthly_max['year_month'] = monthly_max['year_month'].dt.to_timestamp('M')\n",
    "    return monthly_max\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    maxret = get_maxret(stock_data, stock)\n",
    "    training_feature_df[stock] = pd.merge(training_feature_df[stock], maxret, how='left', left_on='date', right_on='year_month')\n",
    "    training_feature_df[stock].drop(columns=['year_month'], inplace=True)\n",
    "    training_feature_df[stock][f'{stock}_maxret'] = training_feature_df[stock][f'{stock}_maxret'].bfill()\n",
    "\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_maxret']):\n",
    "        print(f\"Feature {stock}_maxret contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_maxret')\n",
    "\n",
    "# Testing\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    maxret = get_maxret(stock_data, stock)\n",
    "    testing_feature_df[stock] = pd.merge(testing_feature_df[stock], maxret, how='left', left_on='date', right_on='year_month')\n",
    "    testing_feature_df[stock].drop(columns=['year_month'], inplace=True)\n",
    "    testing_feature_df[stock][f'{stock}_maxret'] = testing_feature_df[stock][f'{stock}_maxret'].bfill()\n",
    "\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_maxret']):\n",
    "        print(f\"Feature {stock}_maxret contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_maxret')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for stock, weird_columns in training_weird_columns_dict.items():\n",
    "    if weird_columns:\n",
    "        training_feature_df[stock] = handle_weird_data_with_knn_imputer(training_feature_df[stock], weird_columns)\n",
    "\n",
    "# Testing\n",
    "for stock, weird_columns in testing_weird_columns_dict.items():\n",
    "    if weird_columns:\n",
    "        testing_feature_df[stock] = handle_weird_data_with_knn_imputer(testing_feature_df[stock], weird_columns)\n",
    "\n",
    "# Check the data again\n",
    "for stock, stock_data in training_feature_df.items():\n",
    "    if check_weird_data(stock_data):\n",
    "        print(f\"The {stock} stock price data still contains weird data in training data\")\n",
    "\n",
    "for stock, stock_data in testing_feature_df.items():\n",
    "    if check_weird_data(stock_data):\n",
    "        print(f\"The {stock} stock price data still contains weird data in testing data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom12m  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m  AAPL_ami_126d  AAPL_age  AAPL_chmom  AAPL_maxret\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500                   0.184606                    0.193321                    0.200102              0.006554               0.006258               0.006046     0.347234    0.026148    0.169584   23.804209       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143                   0.184606                    0.193321                    0.200102             -0.050896              -0.048602              -0.046955     0.347234    0.026148    0.169584   23.784508       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531                   0.184606                    0.193321                    0.200102              0.030740               0.029354               0.028360     0.347234    0.026148    0.169584   23.790760       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631                   0.184606                    0.193321                    0.200102              0.023195               0.022149               0.021398     0.347234    0.026148    0.169584   23.787023       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759                   0.184606                    0.193321                    0.200102              0.041727               0.039846               0.038496     0.347234    0.026148    0.169584   23.794590       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m  AAPL_ami_126d  AAPL_age  AAPL_maxret\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000                   0.110874                    0.116524                    0.117283              0.014814               0.014095               0.014004    0.034747    0.154754   26.149610       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644                   0.110874                    0.116524                    0.117283              0.091493               0.087057               0.086494    0.034747    0.154754   26.187804       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623                   0.110874                    0.116524                    0.117283             -0.097179              -0.092467              -0.091869    0.034747    0.154754   26.179840       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726                   0.110874                    0.116524                    0.117283              0.330321               0.314304               0.312272    0.034747    0.154754   26.265743       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025                   0.110874                    0.116524                    0.117283              0.035345               0.033631               0.033414    0.034747    0.154754   26.269987       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794\n"
     ]
    }
   ],
   "source": [
    "# Clear weird_columns_dict, only clear the value, not the key\n",
    "for stock in stock_list:\n",
    "    training_weird_columns_dict[stock] = []\n",
    "\n",
    "for stock in stock_list:\n",
    "    testing_weird_columns_dict[stock] = []\n",
    "\n",
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 23: Dollar trading volume (dolvol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dolvol(stock):\n",
    "    gobal_factor = pd.read_excel(f\"{DATA_DIR}/global_factor/{stock}_global_factor.xlsx\")\n",
    "    target_cols = ['Day of last price observation (date)', 'Dollar trading volume (dolvol)']\n",
    "    dolvol = gobal_factor[target_cols].copy()\n",
    "    \n",
    "\n",
    "    # Rename the columns\n",
    "    dolvol.rename(columns={'Dollar trading volume (dolvol)': f'{stock}_dolvol'}, inplace=True)\n",
    "\n",
    "    # Parse the date column\n",
    "    dolvol['Day of last price observation (date)'] = parse_dates(dolvol['Day of last price observation (date)'])\n",
    "\n",
    "    return dolvol\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    dolvol = get_dolvol(stock)\n",
    "\n",
    "    # Merge the dolvol data with the stock data\n",
    "    training_feature_df[stock] = pd.merge(training_feature_df[stock], dolvol, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    training_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    training_feature_df[stock][f'{stock}_dolvol'] = training_feature_df[stock][f'{stock}_dolvol'].bfill()\n",
    "\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_dolvol']):\n",
    "        print(f\"Feature f'{stock}_dolvol' contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_dolvol')\n",
    "\n",
    "# Testing\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    dolvol = get_dolvol(stock)\n",
    "\n",
    "    # Merge the dolvol data with the stock data\n",
    "    testing_feature_df[stock] = pd.merge(testing_feature_df[stock], dolvol, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    testing_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    testing_feature_df[stock][f'{stock}_dolvol'] = testing_feature_df[stock][f'{stock}_dolvol'].bfill()\n",
    "\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_dolvol']):\n",
    "        print(f\"Feature f'{stock}_dolvol' contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_dolvol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom12m  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m  AAPL_ami_126d  AAPL_age  AAPL_chmom  AAPL_maxret   AAPL_dolvol\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500                   0.184606                    0.193321                    0.200102              0.006554               0.006258               0.006046     0.347234    0.026148    0.169584   23.804209       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143                   0.184606                    0.193321                    0.200102             -0.050896              -0.048602              -0.046955     0.347234    0.026148    0.169584   23.784508       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531                   0.184606                    0.193321                    0.200102              0.030740               0.029354               0.028360     0.347234    0.026148    0.169584   23.790760       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631                   0.184606                    0.193321                    0.200102              0.023195               0.022149               0.021398     0.347234    0.026148    0.169584   23.787023       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759                   0.184606                    0.193321                    0.200102              0.041727               0.039846               0.038496     0.347234    0.026148    0.169584   23.794590       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m  AAPL_ami_126d  AAPL_age  AAPL_maxret   AAPL_dolvol\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000                   0.110874                    0.116524                    0.117283              0.014814               0.014095               0.014004    0.034747    0.154754   26.149610       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644                   0.110874                    0.116524                    0.117283              0.091493               0.087057               0.086494    0.034747    0.154754   26.187804       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623                   0.110874                    0.116524                    0.117283             -0.097179              -0.092467              -0.091869    0.034747    0.154754   26.179840       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726                   0.110874                    0.116524                    0.117283              0.330321               0.314304               0.312272    0.034747    0.154754   26.265743       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025                   0.110874                    0.116524                    0.117283              0.035345               0.033631               0.033414    0.034747    0.154754   26.269987       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11\n"
     ]
    }
   ],
   "source": [
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 24: 60 Month CAPM Beta (beta_60m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta_60m(stock):\n",
    "    global_factor = pd.read_excel(f\"{DATA_DIR}/global_factor/{stock}_global_factor.xlsx\")\n",
    "    target_cols = ['Day of last price observation (date)', 'Market Beta (beta_60m)']\n",
    "    beta_60m = global_factor[target_cols].copy()\n",
    "    \n",
    "\n",
    "    # Rename the columns\n",
    "    beta_60m.rename(columns={'Market Beta (beta_60m)': f'{stock}_beta_60m'}, inplace=True)\n",
    "\n",
    "    # Parse the date column\n",
    "    beta_60m['Day of last price observation (date)'] = parse_dates(beta_60m['Day of last price observation (date)'])\n",
    "\n",
    "    return beta_60m\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    beta_60m = get_beta_60m(stock)\n",
    "\n",
    "    # Merge the beta_60m data with the stock data\n",
    "    training_feature_df[stock] = pd.merge(training_feature_df[stock], beta_60m, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    training_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    training_feature_df[stock][f'{stock}_beta_60m'] = training_feature_df[stock][f'{stock}_beta_60m'].bfill()\n",
    "\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_beta_60m']):\n",
    "        print(f\"Feature f'{stock}_beta_60m' contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_beta_60m')\n",
    "\n",
    "# Testing\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    beta_60m = get_beta_60m(stock)\n",
    "\n",
    "    # Merge the beta_60m data with the stock data\n",
    "    testing_feature_df[stock] = pd.merge(testing_feature_df[stock], beta_60m, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    testing_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    testing_feature_df[stock][f'{stock}_beta_60m'] = testing_feature_df[stock][f'{stock}_beta_60m'].bfill()\n",
    "\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_beta_60m']):\n",
    "        print(f\"Feature f'{stock}_beta_60m' contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_beta_60m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom12m  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m  AAPL_ami_126d  AAPL_age  AAPL_chmom  AAPL_maxret   AAPL_dolvol  AAPL_beta_60m\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500                   0.184606                    0.193321                    0.200102              0.006554               0.006258               0.006046     0.347234    0.026148    0.169584   23.804209       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10       0.982327\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143                   0.184606                    0.193321                    0.200102             -0.050896              -0.048602              -0.046955     0.347234    0.026148    0.169584   23.784508       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10       0.982327\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531                   0.184606                    0.193321                    0.200102              0.030740               0.029354               0.028360     0.347234    0.026148    0.169584   23.790760       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10       0.982327\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631                   0.184606                    0.193321                    0.200102              0.023195               0.022149               0.021398     0.347234    0.026148    0.169584   23.787023       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10       0.982327\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759                   0.184606                    0.193321                    0.200102              0.041727               0.039846               0.038496     0.347234    0.026148    0.169584   23.794590       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10       0.982327\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m  AAPL_ami_126d  AAPL_age  AAPL_maxret   AAPL_dolvol  AAPL_beta_60m\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000                   0.110874                    0.116524                    0.117283              0.014814               0.014095               0.014004    0.034747    0.154754   26.149610       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11       1.206879\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644                   0.110874                    0.116524                    0.117283              0.091493               0.087057               0.086494    0.034747    0.154754   26.187804       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11       1.206879\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623                   0.110874                    0.116524                    0.117283             -0.097179              -0.092467              -0.091869    0.034747    0.154754   26.179840       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11       1.206879\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726                   0.110874                    0.116524                    0.117283              0.330321               0.314304               0.312272    0.034747    0.154754   26.265743       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11       1.206879\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025                   0.110874                    0.116524                    0.117283              0.035345               0.033631               0.033414    0.034747    0.154754   26.269987       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11       1.206879\n"
     ]
    }
   ],
   "source": [
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 25-27: Number of zero trades with turnover as tiebreaker (1 month, 6 months, 12 months) (zero_trades_21d, zero_trades_126d, zero_trades_252d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_trades(stock):\n",
    "    global_factor = pd.read_excel(f\"{DATA_DIR}/global_factor/{stock}_global_factor.xlsx\")\n",
    "    target_cols = ['Day of last price observation (date)', 'Number of zero trades with turnover as tiebreaker (1 month) (ze', 'Number of zero trades with turnover as tiebreaker (6 months) (z', 'Number of zero trades with turnover as tiebreaker (12 months) (' ]\n",
    "    zero_trades = global_factor[target_cols].copy()\n",
    "    \n",
    "\n",
    "    # Rename the columns\n",
    "    zero_trades.rename(columns={\n",
    "        'Number of zero trades with turnover as tiebreaker (1 month) (ze': f'{stock}_zero_trades_1m', \n",
    "        'Number of zero trades with turnover as tiebreaker (6 months) (z': f'{stock}_zero_trades_6m', \n",
    "        'Number of zero trades with turnover as tiebreaker (12 months) (': f'{stock}_zero_trades_12m'}, inplace=True)\n",
    "\n",
    "    # Parse the date column\n",
    "    zero_trades['Day of last price observation (date)'] = parse_dates(zero_trades['Day of last price observation (date)'])\n",
    "\n",
    "    return zero_trades\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    zero_trades = get_zero_trades(stock)\n",
    "\n",
    "    # Merge the zero_trades data with the stock data\n",
    "    training_feature_df[stock] = pd.merge(training_feature_df[stock], zero_trades, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    training_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    training_feature_df[stock][f'{stock}_zero_trades_1m'] = training_feature_df[stock][f'{stock}_zero_trades_1m'].bfill()\n",
    "    training_feature_df[stock][f'{stock}_zero_trades_6m'] = training_feature_df[stock][f'{stock}_zero_trades_6m'].bfill()\n",
    "    training_feature_df[stock][f'{stock}_zero_trades_12m'] = training_feature_df[stock][f'{stock}_zero_trades_12m'].bfill()\n",
    "\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_zero_trades_1m']):\n",
    "        print(f\"Feature f'{stock}_zero_trades_1m' contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_zero_trades_1m')\n",
    "\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_zero_trades_6m']):\n",
    "        print(f\"Feature f'{stock}_zero_trades_6m' contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_zero_trades_6m')\n",
    "\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_zero_trades_12m']):\n",
    "        print(f\"Feature f'{stock}_zero_trades_12m' contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_zero_trades_12m')\n",
    "\n",
    "# Testing\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    zero_trades = get_zero_trades(stock)\n",
    "\n",
    "    # Merge the zero_trades data with the stock data\n",
    "    testing_feature_df[stock] = pd.merge(testing_feature_df[stock], zero_trades, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    testing_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    testing_feature_df[stock][f'{stock}_zero_trades_1m'] = testing_feature_df[stock][f'{stock}_zero_trades_1m'].bfill()\n",
    "    testing_feature_df[stock][f'{stock}_zero_trades_6m'] = testing_feature_df[stock][f'{stock}_zero_trades_6m'].bfill()\n",
    "    testing_feature_df[stock][f'{stock}_zero_trades_12m'] = testing_feature_df[stock][f'{stock}_zero_trades_12m'].bfill()\n",
    "\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_zero_trades_1m']):\n",
    "        print(f\"Feature f'{stock}_zero_trades_1m' contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_zero_trades_1m')\n",
    "\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_zero_trades_6m']):\n",
    "        print(f\"Feature f'{stock}_zero_trades_6m' contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_zero_trades_6m')\n",
    "\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_zero_trades_12m']):\n",
    "        print(f\"Feature f'{stock}_zero_trades_12m' contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_zero_trades_12m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom12m  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m  AAPL_ami_126d  AAPL_age  AAPL_chmom  AAPL_maxret   AAPL_dolvol  AAPL_beta_60m  AAPL_zero_trades_1m  AAPL_zero_trades_6m  AAPL_zero_trades_12m\n",
      "0 2016-07-01                          23.972500                           23.972500                           23.972500                   0.184606                    0.193321                    0.200102              0.006554               0.006258               0.006046     0.347234    0.026148    0.169584   23.804209       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10       0.982327             0.002349             0.002308              0.002108\n",
      "1 2016-07-05                          23.852214                           23.856102                           23.858143                   0.184606                    0.193321                    0.200102             -0.050896              -0.048602              -0.046955     0.347234    0.026148    0.169584   23.784508       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10       0.982327             0.002349             0.002308              0.002108\n",
      "2 2016-07-06                          23.863737                           23.865518                           23.866531                   0.184606                    0.193321                    0.200102              0.030740               0.029354               0.028360     0.347234    0.026148    0.169584   23.790760       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10       0.982327             0.002349             0.002308              0.002108\n",
      "3 2016-07-07                          23.900616                           23.898562                           23.897631                   0.184606                    0.193321                    0.200102              0.023195               0.022149               0.021398     0.347234    0.026148    0.169584   23.787023       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10       0.982327             0.002349             0.002308              0.002108\n",
      "4 2016-07-08                          23.970359                           23.960624                           23.955759                   0.184606                    0.193321                    0.200102              0.041727               0.039846               0.038496     0.347234    0.026148    0.169584   23.794590       0.016383       -0.000876       -0.016245       -0.028161       0.000003     439.0   -0.025031     0.064963  7.149022e+10       0.982327             0.002349             0.002308              0.002108\n",
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation_10  AAPL_downside_deviation_21  AAPL_sortino_ratio_5  AAPL_sortino_ratio_10  AAPL_sortino_ratio_21  AAPL_mom1m  AAPL_mom6m  AAPL_mvel1  AAPL_rvol_21d  AAPL_chcsho_1m  AAPL_chcsho_3m  AAPL_chcsho_6m  AAPL_ami_126d  AAPL_age  AAPL_maxret   AAPL_dolvol  AAPL_beta_60m  AAPL_zero_trades_1m  AAPL_zero_trades_6m  AAPL_zero_trades_12m\n",
      "0 2023-01-03                         125.070000                          125.070000                          125.070000                   0.110874                    0.116524                    0.117283              0.014814               0.014095               0.014004    0.034747    0.154754   26.149610       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11       1.206879             0.003414             0.003331              0.003452\n",
      "1 2023-01-04                         125.759637                          125.737345                          125.725644                   0.110874                    0.116524                    0.117283              0.091493               0.087057               0.086494    0.034747    0.154754   26.187804       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11       1.206879             0.003414             0.003331              0.003452\n",
      "2 2023-01-05                         125.478234                          125.481477                          125.482623                   0.110874                    0.116524                    0.117283             -0.097179              -0.092467              -0.091869    0.034747    0.154754   26.179840       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11       1.206879             0.003414             0.003331              0.003452\n",
      "3 2023-01-06                         126.737832                          126.626030                          126.568726                   0.110874                    0.116524                    0.117283              0.330321               0.314304               0.312272    0.034747    0.154754   26.265743       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11       1.206879             0.003414             0.003331              0.003452\n",
      "4 2023-01-09                         127.621237                          127.431748                          127.333025                   0.110874                    0.116524                    0.117283              0.035345               0.033631               0.033414    0.034747    0.154754   26.269987       0.016254       -0.001292       -0.005417       -0.015482       0.000001     517.0     0.036794  2.081135e+11       1.206879             0.003414             0.003331              0.003452\n"
     ]
    }
   ],
   "source": [
    "# Print the result to ensure the data is correct\n",
    "print(training_feature_df['AAPL'].head().to_string())\n",
    "print(testing_feature_df['AAPL'].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 13: Frazzini-Pedersen market beta (betabab_1260d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_betabab_1260d(stock):\n",
    "    global_factor = pd.read_excel(f\"{DATA_DIR}/global_factor/{stock}_global_factor.xlsx\")\n",
    "    target_cols = ['Day of last price observation (date)', 'Frazzini-Pedersen market beta (betabab_1260d)']\n",
    "    betabab_1260d = global_factor[target_cols].copy()\n",
    "    \n",
    "\n",
    "    # Rename the columns\n",
    "    betabab_1260d.rename(columns={'Frazzini-Pedersen market beta (betabab_1260d)': f'{stock}_betabab_1260d'}, inplace=True)\n",
    "\n",
    "    # Parse the date column\n",
    "    betabab_1260d['Day of last price observation (date)'] = parse_dates(betabab_1260d['Day of last price observation (date)'])\n",
    "\n",
    "    return betabab_1260d\n",
    "\n",
    "# Training\n",
    "for stock, stock_data in training_stock_df.items():\n",
    "    betabab_1260d = get_betabab_1260d(stock)\n",
    "\n",
    "    # Merge the data with the stock data\n",
    "    training_feature_df[stock] = pd.merge(training_feature_df[stock], betabab_1260d, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    training_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    training_feature_df[stock][f'{stock}_betabab_1260d'] = training_feature_df[stock][f'{stock}_betabab_1260d'].bfill()\n",
    "\n",
    "    if check_weird_data(training_feature_df[stock][f'{stock}_betabab_1260d']):\n",
    "        print(f\"Feature f'{stock}_betabab_1260d' contains weird data in training data\")\n",
    "        training_weird_columns_dict[stock].append(f'{stock}_betabab_1260d')\n",
    "\n",
    "# Testing\n",
    "for stock, stock_data in testing_stock_df.items():\n",
    "    betabab_1260d = get_betabab_1260d(stock)\n",
    "\n",
    "    # Merge the data with the stock data\n",
    "    testing_feature_df[stock] = pd.merge(testing_feature_df[stock], betabab_1260d, how='left', left_on='date', right_on='Day of last price observation (date)')\n",
    "    # Drop the 'Day of last price observation (date)' column\n",
    "    testing_feature_df[stock].drop(columns=['Day of last price observation (date)'], inplace=True)\n",
    "\n",
    "    # Backfill the missing values\n",
    "    testing_feature_df[stock][f'{stock}_betabab_1260d'] = testing_feature_df[stock][f'{stock}_betabab_1260d'].bfill()\n",
    "\n",
    "    if check_weird_data(testing_feature_df[stock][f'{stock}_betabab_1260d']):\n",
    "        print(f\"Feature f'{stock}_betabab_1260d' contains weird data in testing data\")\n",
    "        testing_weird_columns_dict[stock].append(f'{stock}_betabab_1260d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  AAPL_exponential_moving_average_5  AAPL_exponential_moving_average_10  AAPL_exponential_moving_average_21  AAPL_downside_deviation_5  AAPL_downside_deviation